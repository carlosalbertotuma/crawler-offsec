# Crawler OffSec

![image](https://github.com/user-attachments/assets/291bc783-c3b0-49e4-a48a-fd45ee6d60ed)

Este √© um crawler desenvolvido para coletar links de websites e subdom√≠nios usando o wget2. O script realiza a varredura recursiva de um dom√≠nio, identifica URLs e subdom√≠nios, e os salva em arquivos de texto para an√°lise posterior.

# Descri√ß√£o
Este script foi desenvolvido para automatizar o processo de varredura de websites e subdom√≠nios, coletando URLs importantes que podem ser √∫teis para testes de seguran√ßa e auditorias. Ele utiliza o wget2, uma ferramenta r√°pida e robusta para baixar e verificar sites de forma eficiente.

# Funcionalidades
Varredura Recursiva: O script utiliza o wget2 com a op√ß√£o --spider para realizar a varredura de um site e identificar links internos.
Identifica√ß√£o de Subdom√≠nios: Ele detecta subdom√≠nios e URLs interessantes que pertencem ao dom√≠nio principal.
Armazenamento de Resultados: Os URLs coletados s√£o salvos em arquivos .txt para an√°lise posterior.

# Pr√©-requisitos
Para rodar o script, voc√™ precisa ter o wget2 instalado em seu sistema. Caso n√£o tenha o wget2 instalado, voc√™ pode instal√°-lo com:

# Para sistemas baseados em Debian/Ubuntu:
sudo apt update
sudo apt install wget2

# Como usar

Passo 1: Baixe o script
Clone o reposit√≥rio onde o script est√° armazenado:

git clone https://github.com/carlosalbertotuma/crawler-offsec.git
cd crawler-offsec
Ou, se preferir, crie um arquivo .sh e cole o script nele.

Passo 2: Torne o script execut√°vel
chmod +x crawler-offsec.sh

Passo 3: Execute o script
Para rodar o script, basta passar o dom√≠nio como par√¢metro, como neste exemplo:

./crawler-offsec.sh example.com
Isso far√° o script processar o dom√≠nio example.com, onde:

Ele ir√° varrer o site usando wget2 e buscar URLs.
Ele cria um arquivo de sa√≠da example.com.txt com os URLs coletados.
Depois, ele busca subdom√≠nios e adiciona os resultados em outro arquivo example.com_full.txt.

Passo 4: Arquivos gerados
example.com.txt: Cont√©m URLs encontradas durante o processo de varredura.
example.com_full.txt: Cont√©m URLs completas e subdom√≠nios descobertos durante a segunda varredura.
Como funciona


## ü§ù Colaboradores

Agradecemos √†s seguintes pessoas que contribu√≠ram para este projeto:

Desenvolvido por Carlos Tuma - Bl4dSc4n


## üìù Licen√ßa

Esse projeto est√° sob √© de livre uso e modifica√ß√£o, favor manter os cr√©ditos em coment√°rio.
 
Ps. n√£o utilize para crimes ciberneticos, n√£o tenho respons√°bilidade do mau uso da ferramenta.

[‚¨Ü Voltar ao topo](#nome-do-projeto)<br>


